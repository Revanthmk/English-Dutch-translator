{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36b4271",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "English - Dutch (Domain Specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bbe9a",
   "metadata": {},
   "source": [
    "#### Going thourgh the validation data\n",
    "\n",
    "The text conatins \n",
    "1. non-natural language text\n",
    "2. Characters like {1}, {2}....\n",
    "3. Units like 30W, 64MP\n",
    "4. Companies like TurboPower^(TM)\n",
    "\n",
    "#### 1. Training Dataset - WMT 2016 EN-Dutch\n",
    "#### 2. General domain evaluation - FLORES devtest\n",
    "#### 3. Validation - Dataset_challenge_1.xlsx\n",
    "\n",
    "## Proposed Steps\n",
    "1. Start with pretrained EN-Dutch translation model\n",
    "2. Finetune on WMT 2016 dataset to adpot it to the IT domain\n",
    "3. Preprocessing text to preserve the characters in validation set\n",
    "4. Validate general domain translation on FLORES dataset\n",
    "5. Validate excel dataset for IT domain\n",
    "\n",
    "The idea is to build a robust pipeline starting with the encoder-decoder model containing data loading, model training, model evaluation using both the methods FLORES and the excel dataset then robustness of the pipeline should let us swap the model and train the decoder-only model. This will let us identify bugs faster in the decoder-only model and identify inconsistencies\n",
    "Dataset loading, Train/val/test split, evaluation methods stays fixed in the pipeline for both the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94552783",
   "metadata": {},
   "source": [
    "### Step 1 - Working on the WMT dataset\n",
    "\n",
    "Looking at the website (https://www.statmt.org/wmt16/it-translation-task.html)\n",
    "I choose out-of-domain training data - Europarl corpus\n",
    "\n",
    "The reason for using this is that we train the model to evaluate if domain adaptaion is achieved from finetuning rather than training it on IT specific data for the model to memorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1d7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_parallel_corpus(en_path, nl_path, sample_k=5):\n",
    "    en_lines = Path(en_path).read_text(encoding=\"utf-8\").splitlines()\n",
    "    nl_lines = Path(nl_path).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    print(f\"Number of English lines: {len(en_lines)}\")\n",
    "    print(f\"NNumer of Dutch lines:   {len(nl_lines)}\")\n",
    "    assert len(en_lines) == len(nl_lines), \"The numbers dont match\"\n",
    "    print()\n",
    "\n",
    "    en_lengths = [len(l.split()) for l in en_lines if l.strip()]\n",
    "    nl_lengths = [len(l.split()) for l in nl_lines if l.strip()]\n",
    "\n",
    "    print(\"----Lengths----\")\n",
    "    print(f\"EN avg/median: {statistics.mean(en_lengths):.1f}/{statistics.median(en_lengths)}\")\n",
    "    print(f\"NL avg/median: {statistics.mean(nl_lengths):.1f}/{statistics.median(nl_lengths)}\")\n",
    "    print(f\"EN max length: {max(en_lengths)}\")\n",
    "    print(f\"NL max length: {max(nl_lengths)}\")\n",
    "    print()\n",
    "\n",
    "    empty_en = sum(1 for l in en_lines if not l.strip())\n",
    "    empty_nl = sum(1 for l in nl_lines if not l.strip())\n",
    "    print(\"-----Empty lines-----\")\n",
    "    print(f\"Empty EN lines: {empty_en}\")\n",
    "    print(f\"Empty NL lines: {empty_nl}\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Sample sentence pairs ---\")\n",
    "    for i in range(sample_k):\n",
    "        print(f\"EN: {en_lines[i]}\")\n",
    "        print(f\"NL: {nl_lines[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path=\"data/nl-en/europarl-v7.nl-en.en\"\n",
    "nl_path=\"data/nl-en/europarl-v7.nl-en.nl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "751ba88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English lines: 1997775\n",
      "NNumer of Dutch lines:   1997775\n",
      "\n",
      "----Lengths----\n",
      "EN avg/median: 25.0 / 22\n",
      "NL avg/median: 25.4 / 23\n",
      "EN max length: 668\n",
      "NL max length: 583\n",
      "\n",
      "-----Empty lines-----\n",
      "Empty EN lines: 15638\n",
      "Empty NL lines: 3392\n",
      "\n",
      "--- Sample sentence pairs ---\n",
      "EN: Resumption of the session\n",
      "NL: Hervatting van de zitting\n",
      "EN: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "NL: Ik verklaar de zitting van het Europees Parlement, die op vrijdag 17 december werd onderbroken, te zijn hervat. Ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad.\n",
      "EN: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "NL: Zoals u heeft kunnen constateren, is de grote \"millenniumbug\" uitgebleven. De burgers van een aantal van onze lidstaten zijn daarentegen door verschrikkelijke natuurrampen getroffen.\n",
      "EN: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "NL: U heeft aangegeven dat u deze vergaderperiode een debat wilt over deze rampen.\n",
      "EN: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "NL: Nu wil ik graag op verzoek van een aantal collega's een minuut stilte in acht nemen ter nagedachtenis van de slachtoffers. Ik doel hiermee met name op de slachtoffers van het noodweer dat verschillende lidstaten van de Unie heeft geteisterd.\n"
     ]
    }
   ],
   "source": [
    "inspect_parallel_corpus(\n",
    "    en_path=en_path,\n",
    "    nl_path=nl_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1c5bb",
   "metadata": {},
   "source": [
    "1. The line pairs match\n",
    "2. Avg length and Median length looks fine around 25\n",
    "3. Max length is around 600 (should look into this model might not support)\n",
    "4. Lot of empty lines\n",
    "\n",
    "TODO:\n",
    "1. Remove empty sentence pairs\n",
    "2. Filter by max length\n",
    "3. Fix a number for finetuning hardware limitation for around 2M pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7df557ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8412e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parallel(en_path, nl_path):\n",
    "    en_lines = Path(en_path).read_text(encoding=\"utf-8\").splitlines()\n",
    "    nl_lines = Path(nl_path).read_text(encoding=\"utf-8\").splitlines()\n",
    "    assert len(en_lines) == len(nl_lines)\n",
    "    return list(zip(en_lines, nl_lines))\n",
    "\n",
    "\n",
    "def filter_pairs(pairs, max_len=128, min_len=3):\n",
    "    filtered = []\n",
    "    for en, nl in pairs:\n",
    "        if not en.strip() or not nl.strip():\n",
    "            continue\n",
    "\n",
    "        en_len = len(en.split())\n",
    "        nl_len = len(nl.split())\n",
    "\n",
    "        if en_len < min_len or nl_len < min_len:\n",
    "            continue\n",
    "        if en_len > max_len or nl_len > max_len:\n",
    "            continue\n",
    "\n",
    "        filtered.append((en, nl))\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def subsample_and_split(pairs, total_size=200_000, val_size=10_000, seed=42):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    pairs = pairs[:total_size]\n",
    "    train = pairs[:-val_size]\n",
    "    val = pairs[-val_size:]\n",
    "\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06731c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num original pairs: 1997775\n",
      "num after filtering: 1959526\n",
      "\n",
      "train size: 190000\n",
      "val size:   10000\n"
     ]
    }
   ],
   "source": [
    "pairs = load_parallel(\n",
    "    en_path,\n",
    "    nl_path\n",
    ")\n",
    "\n",
    "print(f\"num original pairs: {len(pairs)}\")\n",
    "\n",
    "filtered_pairs = filter_pairs(pairs)\n",
    "print(f\"num after filtering: {len(filtered_pairs)}\")\n",
    "print()\n",
    "\n",
    "train_pairs, val_pairs = subsample_and_split(filtered_pairs)\n",
    "\n",
    "print(f\"train size: {len(train_pairs)}\")\n",
    "print(f\"val size:   {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c427549",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path=\"data/nl-en/europarl-v7.nl-en.en\"\n",
    "nl_path=\"data/nl-en/europarl-v7.nl-en.nl\"\n",
    "\n",
    "train_path_en = \"data/train.en\"\n",
    "val_path_en = \"data/val.en\"\n",
    "\n",
    "train_path_nl = \"data/train.nl\"\n",
    "val_path_nl = \"data/val.nl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bdda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pairs(pairs, out_en, out_nl):\n",
    "    Path(out_en).write_text(\"\\n\".join(p[0] for p in pairs), encoding=\"utf-8\")\n",
    "    Path(out_nl).write_text(\"\\n\".join(p[1] for p in pairs), encoding=\"utf-8\")\n",
    "\n",
    "save_pairs(train_pairs, train_path_en, train_path_nl)\n",
    "save_pairs(val_pairs, val_path_en, val_path_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a5016",
   "metadata": {},
   "source": [
    "1. Setting max length to 128 because most small models support only upto 128 tokens only around 40000 lines are skipped so that shouldn't be an issue\n",
    "2. Sampling 190k pairs for training and 10k for validation should be enough to finetune a small encoder-decoder model\n",
    "3. Saved the train and val data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fef4ea",
   "metadata": {},
   "source": [
    "### Step 2 - Working on the Encoder-Decoder model\n",
    "\n",
    "\n",
    "1. Choose the encoder-decoder base model\n",
    "\n",
    "1a - from Europarl the average length is 25 words \n",
    "\n",
    "1b - we have sentence level parallel data\n",
    "\n",
    "1c - We can choose MarianMT as our base model\n",
    "\n",
    "1ci - small enough to fine tune on arond 200k pairs\n",
    "\n",
    "1cii - Trained specifically on EN-Dutch language\n",
    "\n",
    "1ciii - Already trained on Europarl\n",
    "\n",
    "1civ - Since it is already trained on Europarl we are continually finetuning to shift hte model's behavior, we are shifiting the model's domain instead of teach the model from start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d808177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8d0b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, tokenizer, max_len=128):\n",
    "        self.src = open(src_file, encoding=\"utf-8\").read().splitlines()\n",
    "        self.tgt = open(tgt_file, encoding=\"utf-8\").read().splitlines()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenizer(\n",
    "            self.src[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tgt = self.tokenizer(\n",
    "            self.tgt[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = tgt[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": src[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels.squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1c81ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-breeze-3</strong> at: <a href='https://wandb.ai/revanthmk/English-Dutch-MT/runs/pvlk3wzs' target=\"_blank\">https://wandb.ai/revanthmk/English-Dutch-MT/runs/pvlk3wzs</a><br> View project at: <a href='https://wandb.ai/revanthmk/English-Dutch-MT' target=\"_blank\">https://wandb.ai/revanthmk/English-Dutch-MT</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260119_182049-pvlk3wzs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ganesh\\Desktop\\Revanth\\Assignment\\LLM company\\wandb\\run-20260119_182749-ent4su1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/revanthmk/English-Dutch-MT/runs/ent4su1h' target=\"_blank\">breezy-grass-4</a></strong> to <a href='https://wandb.ai/revanthmk/English-Dutch-MT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/revanthmk/English-Dutch-MT' target=\"_blank\">https://wandb.ai/revanthmk/English-Dutch-MT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/revanthmk/English-Dutch-MT/runs/ent4su1h' target=\"_blank\">https://wandb.ai/revanthmk/English-Dutch-MT/runs/ent4su1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;wandb.sdk.wandb_run.Run object at 0x000001CAE8E80A10&gt;"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1cae8e80a10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"English-Dutch-MT\",\n",
    "    config={\n",
    "        \"model\": \"Helsinki-NLP\",\n",
    "        \"lr\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"max_len\": 128,\n",
    "        \"epochs\": 3,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0a1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ec39a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganesh\\Desktop\\Revanth\\Assignment\\LLM company\\rough_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-nl\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=wandb.config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e5c999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0c6174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TranslationDataset(train_path_en, train_path_nl, tokenizer)\n",
    "val_ds = TranslationDataset(val_path_en, val_path_nl, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfc811d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 1.3439366612785741, val loss: 0.9585955175399781\n",
      "epoch 1, train loss: 0.9716918534680417, val loss: 0.8753711093902588\n",
      "epoch 2, train loss: 0.8807454771242643, val loss: 0.8412576155662537\n"
     ]
    }
   ],
   "source": [
    "for each_epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {\n",
    "            k: v.to(device) for k, v in batch.items()\n",
    "        }\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    wandb.log({\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"epoch\": each_epoch\n",
    "    })\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {\n",
    "                k: v.to(device) for k, v in batch.items()\n",
    "            }\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    avg_val_loss = val_loss/len(val_loader)\n",
    "    wandb.log({\n",
    "        \"val_loss\": avg_val_loss,\n",
    "    })\n",
    "\n",
    "    print(f\"epoch {each_epoch}, train loss: {avg_train_loss}, val loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned-marian-en-nl\")\n",
    "tokenizer.save_pretrained(\"finetuned-marian-en-nl\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953b664",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abb3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdbc42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganesh\\Desktop\\Revanth\\Assignment\\LLM company\\rough_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"finetuned-marian-en-nl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765c65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, max_len=128):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=max_len\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_len,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ecf737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_sentences, gt_sentences):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, text in enumerate(x_sentences):\n",
    "        translation = translate(text)\n",
    "        all_predictions.append(translation)\n",
    "\n",
    "    refs_list = gt_sentences\n",
    "    bleu = sacrebleu.corpus_bleu(all_predictions, refs_list, tokenize=\"flores200\")\n",
    "    chrf = sacrebleu.corpus_chrf(all_predictions, refs_list, word_order=2)\n",
    "    ter = sacrebleu.corpus_ter(all_predictions, refs_list)\n",
    "\n",
    "    print(f\"SacreBLEU: {bleu.score:.2f}\")\n",
    "    print(f\"chrF++:    {chrf.score:.2f}\")\n",
    "    print(f\"TER:       {ter.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10298bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flores(flores_path):\n",
    "    en_df = pd.read_parquet(flores_path+\"eng_Latn.parquet\", engine=\"fastparquet\")\n",
    "    nl_df = pd.read_parquet(flores_path+\"nld_Latn.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "    x_sentences = en_df[\"text\"].tolist()\n",
    "    gt_sentences = nl_df[\"text\"].tolist()\n",
    "    return x_sentences, gt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcec9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_excel(csv_path):\n",
    "    df = pd.read_excel(csv_path)\n",
    "    return df['English Source'].tolist(), df['Reference Translation'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f110ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU: 0.05\n",
      "chrF++:    7.34\n",
      "TER:       3003.71\n"
     ]
    }
   ],
   "source": [
    "x_sentences, gt_sentences = load_flores(\"data/flores/\")\n",
    "evaluate(x_sentences, gt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96c46139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU: 0.18\n",
      "chrF++:    5.83\n",
      "TER:       963.06\n"
     ]
    }
   ],
   "source": [
    "x_sentences, gt_sentences = load_challenge_excel(\"Dataset_Challenge_1.xlsx\")\n",
    "evaluate(model, x_sentences, gt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab3595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da68242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41401bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef02939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4733d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4abf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c061fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28355746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab6a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077e045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113be3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6871d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1e34ae",
   "metadata": {},
   "source": [
    "Okay so the model training is done took around 4hrs and lets check if the model does any better than the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03fbf5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganesh\\Desktop\\Revanth\\Assignment\\LLM company\\rough_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(67028, 512, padding_idx=67027)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=67028, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = \"finetuned-marian-en-nl\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-nl\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "base_model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, model, max_len=128):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = base_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_len,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Dataset_Challenge_1.xlsx\")\n",
    "\n",
    "finetuned_translations = [translate(x, model) for x in df['English Source']]\n",
    "base_translations = [translate(x, base_model) for x in df[\"English Source\"]]\n",
    "\n",
    "df[\"finetuned_translations\"] = finetuned_translations\n",
    "df[\"base_translations\"] = base_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23faa01",
   "metadata": {},
   "source": [
    "NOPE - The model performs prettymuch as same as the base model and worse in few cases based on looking at few examples and the evaluation results <br>\n",
    "Which is again expected I guess, the model has no gradient signals how to translate {1} or built-inUFS spacing <br>\n",
    "Finetuning on out of domain data alone isnt enough to adapt the translation to the required domain<br>\n",
    "In few cases like {1} the base model does better than the finetuned model\n",
    "\n",
    "What to do next? <br>\n",
    "We can go about in 2 ways <br>\n",
    "1. Freeze encoder and finetune only the decoder for lower learning rate and fewer epochs - this can reduce the domain overshifting problem we are observing above - I cannot do this because of the time constraint<br>\n",
    "\n",
    "2. Try preprocessing the test data and build guard rails for expected patterns that has to be preserved and the ones which won't probably present in the training dataset <br>\n",
    "Add masks over the text the model will probably not recognize and unmask after generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4883d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PLACEHOLDER_PATTERN = re.compile(r\"\\{\\d+\\}\")\n",
    "SYMBOL_PATTERN = re.compile(r\"[™®]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ab71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_text(text):\n",
    "    mappings = {}\n",
    "    counter = 0\n",
    "\n",
    "    def _replace(match):\n",
    "        nonlocal counter\n",
    "        key = f\"<MASK{counter}>\"\n",
    "        mappings[key] = match.group(0)\n",
    "        counter += 1\n",
    "        return key\n",
    "\n",
    "    text = PLACEHOLDER_PATTERN.sub(_replace, text)\n",
    "    text = SYMBOL_PATTERN.sub(_replace, text)\n",
    "\n",
    "    return text, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ee1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmask_text(text, mappings):\n",
    "    for k, v in mappings.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_mask(text, model, max_len=128):\n",
    "    masked_text, mappings = mask_text(text)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        masked_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=4,\n",
    "            max_length=max_len\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return unmask_text(decoded, mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6072af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(67128, 512, padding_idx=67027)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASK_TOKENS = [f\"<MASK{i}>\" for i in range(100)]\n",
    "\n",
    "tokenizer.add_tokens(MASK_TOKENS)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0915fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Dataset_Challenge_1.xlsx\")\n",
    "\n",
    "finetuned_translations_masked = [translate_mask(x, model) for x in df['English Source']]\n",
    "base_translations_masked = [translate_mask(x, base_model) for x in df[\"English Source\"]]\n",
    "\n",
    "df[\"finetuned_translations_masked\"] = finetuned_translations_masked\n",
    "df[\"base_translations_masked\"] = base_translations_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4e52fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English Source</th>\n",
       "      <th>Reference Translation</th>\n",
       "      <th>finetuned_translations_masked</th>\n",
       "      <th>base_translations_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256GB built-inUFS 3.1 + LPDDR5\"</td>\n",
       "      <td>256 GB ingebouwdUFS 3.1 + LPDDR5</td>\n",
       "      <td>Ingebouwde zin 3,1 + LPDDR5\"</td>\n",
       "      <td>256GB ingebouwdeUFS 3.1 + LPDDR5\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snap crystal-clear photos and selfies in any l...</td>\n",
       "      <td>Maak kristalheldere foto's en selfies bij elk ...</td>\n",
       "      <td>Stelt u glasheldere foto's en zelfieën in iede...</td>\n",
       "      <td>Snap kristalheldere foto's en selfies in elk l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30W TurboPower™ charging support*sold separate...</td>\n",
       "      <td>Ondersteuning voor TurboPower™ laden met 30 W{...</td>\n",
       "      <td>schriftelijk. - (PT) De lidstaten van de Commi...</td>\n",
       "      <td>30W TurboPower™ laadondersteuning* afzonderlij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{1}Update window: From {2} to {3}</td>\n",
       "      <td>{1}Updateperiode: van {2} tot {3}</td>\n",
       "      <td>schriftelijk. - (NL)</td>\n",
       "      <td>{1}Venster bijwerken: Van {2} naar {3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Or leave now and get a reminder.</td>\n",
       "      <td>Of sluit af en ontvangen een herinnering.</td>\n",
       "      <td>Of vertrek nu en krijg een herinnering.</td>\n",
       "      <td>Of vertrek nu en vraag om een herinnering.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      English Source  \\\n",
       "0                    256GB built-inUFS 3.1 + LPDDR5\"   \n",
       "1  Snap crystal-clear photos and selfies in any l...   \n",
       "2  30W TurboPower™ charging support*sold separate...   \n",
       "3                  {1}Update window: From {2} to {3}   \n",
       "4                   Or leave now and get a reminder.   \n",
       "\n",
       "                               Reference Translation  \\\n",
       "0                   256 GB ingebouwdUFS 3.1 + LPDDR5   \n",
       "1  Maak kristalheldere foto's en selfies bij elk ...   \n",
       "2  Ondersteuning voor TurboPower™ laden met 30 W{...   \n",
       "3                  {1}Updateperiode: van {2} tot {3}   \n",
       "4          Of sluit af en ontvangen een herinnering.   \n",
       "\n",
       "                       finetuned_translations_masked  \\\n",
       "0                       Ingebouwde zin 3,1 + LPDDR5\"   \n",
       "1  Stelt u glasheldere foto's en zelfieën in iede...   \n",
       "2  schriftelijk. - (PT) De lidstaten van de Commi...   \n",
       "3                               schriftelijk. - (NL)   \n",
       "4            Of vertrek nu en krijg een herinnering.   \n",
       "\n",
       "                            base_translations_masked  \n",
       "0                  256GB ingebouwdeUFS 3.1 + LPDDR5\"  \n",
       "1  Snap kristalheldere foto's en selfies in elk l...  \n",
       "2  30W TurboPower™ laadondersteuning* afzonderlij...  \n",
       "3             {1}Venster bijwerken: Van {2} naar {3}  \n",
       "4         Of vertrek nu en vraag om een herinnering.  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110bc06",
   "metadata": {},
   "source": [
    "Somehow the finetuned model is removing the <MASK> tokens, hypothesis is that the finetuned model has learned to remove unknown tokens but the base model keeps it intact <br>\n",
    "Fix would be to mask the training data and then finetune the model again but with the interest of time, I couldn't do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932f55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014256c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8fffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db2b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cca68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86219e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a50095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd4e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1590f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53feab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3218bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adefd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460ff28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5baff29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f18b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5603cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253209a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed41b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463011d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252ba43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rough_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
